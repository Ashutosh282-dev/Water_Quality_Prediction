{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Quality Prediction - Week 1\n",
    "## Project Setup and Data Analysis\n",
    "### 1. Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required libraries are imported!\n"
     ]
    }
   ],
   "source": [
    "# Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Settings\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"All required libraries are imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id;date;NH4;BSK5;Suspended;O2;NO3;NO2;SO4;PO4;CL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1;17.02.2000;0.33;2.77;12;12.3;9.5;0.057;154;0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1;11.05.2000;0.044;3;51.6;14.61;17.75;0.034;35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1;11.09.2000;0.032;2.1;24.5;9.87;13.8;0.173;41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1;13.12.2000;0.17;2.23;35.6;12.4;17.13;0.099;2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1;02.03.2001;0;3.03;48.8;14.69;10;0.065;281.6;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id;date;NH4;BSK5;Suspended;O2;NO3;NO2;SO4;PO4;CL\n",
       "0  1;17.02.2000;0.33;2.77;12;12.3;9.5;0.057;154;0...\n",
       "1  1;11.05.2000;0.044;3;51.6;14.61;17.75;0.034;35...\n",
       "2  1;11.09.2000;0.032;2.1;24.5;9.87;13.8;0.173;41...\n",
       "3  1;13.12.2000;0.17;2.23;35.6;12.4;17.13;0.099;2...\n",
       "4  1;02.03.2001;0;3.03;48.8;14.69;10;0.065;281.6;..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2861 entries, 0 to 2860\n",
      "Data columns (total 1 columns):\n",
      " #   Column                                            Non-Null Count  Dtype \n",
      "---  ------                                            --------------  ----- \n",
      " 0   id;date;NH4;BSK5;Suspended;O2;NO3;NO2;SO4;PO4;CL  2861 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 22.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Note: Please upload your dataset to the 'data' folder and update the filename below\n",
    "file_path = 'C:/Users/shali/Desktop/aicte internship folder/afa2e701598d20110228.csv' # Update this with your actual file name\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    display(df.head())\n",
    "    print(\"\\nDataset Info:\")\n",
    "    df.info()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}. Please make sure the file exists.\")\n",
    "    print(\"Typical column names in water quality datasets might include:\")\n",
    "    print(\"- pH, Hardness, Solids, Chloramines, Sulfate, Conductivity, Organic_carbon, \"\n",
    "          \"Trihalomethanes, Turbidity, Potability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values before imputation:\n",
      "id;date;NH4;BSK5;Suspended;O2;NO3;NO2;SO4;PO4;CL    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use mean strategy with non-numeric data:\ncould not convert string to float: '1;17.02.2000;0.33;2.77;12;12.3;9.5;0.057;154;0.454;289.5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Apply preprocessing if dataset is loaded\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n\u001b[1;32m---> 24\u001b[0m     df_clean \u001b[38;5;241m=\u001b[39m preprocess_data(df)\n",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Impute missing values with mean (you can change strategy as needed)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m imputer \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(imputer\u001b[38;5;241m.\u001b[39mfit_transform(df_clean), columns\u001b[38;5;241m=\u001b[39mdf_clean\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 2. Check for duplicates\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNumber of duplicate rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_clean\u001b[38;5;241m.\u001b[39mduplicated()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shali\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\shali\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\shali\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:390\u001b[0m, in \u001b[0;36mSimpleImputer.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    382\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter was deprecated in version \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.1 and will be removed in 1.3. A warning will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    388\u001b[0m     )\n\u001b[1;32m--> 390\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(X, in_fit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;66;03m# otherwise\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\shali\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:342\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n\u001b[0;32m    337\u001b[0m     new_ve \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    338\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m strategy with non-numeric data:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    339\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy, ve\n\u001b[0;32m    340\u001b[0m         )\n\u001b[0;32m    341\u001b[0m     )\n\u001b[1;32m--> 342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ve\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot use mean strategy with non-numeric data:\ncould not convert string to float: '1;17.02.2000;0.33;2.77;12;12.3;9.5;0.057;154;0.454;289.5'"
     ]
    }
   ],
   "source": [
    "def preprocess_data(df):\n",
    "    # Make a copy of the dataframe\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Convert date column to datetime if it exists\n",
    "    if 'date' in df_clean.columns:\n",
    "        df_clean['date'] = pd.to_datetime(df_clean['date'], format='%d.%m.%Y')\n",
    "    \n",
    "    # 2. Convert all columns to numeric, coercing errors to NaN\n",
    "    for col in df_clean.select_dtypes(include=['object']).columns:\n",
    "        if col != 'date':  # Skip date column\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col].str.replace(',', '.'), errors='coerce')\n",
    "    \n",
    "    # 3. Handle missing values\n",
    "    print(\"\\nMissing values before imputation:\")\n",
    "    print(df_clean.isnull().sum())\n",
    "    \n",
    "    # 4. Impute missing values with mean for numeric columns\n",
    "    numeric_cols = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        df_clean[numeric_cols] = imputer.fit_transform(df_clean[numeric_cols])\n",
    "    else:\n",
    "        print(\"No numeric columns found for imputation\")\n",
    "    \n",
    "    # 5. Check for duplicates\n",
    "    print(f\"\\nNumber of duplicate rows: {df_clean.duplicated().sum()}\")\n",
    "    \n",
    "    # 6. Basic statistics\n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    display(df_clean.describe())\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# First, let's load the data correctly\n",
    "file_path = r'C:\\Users\\shali\\Desktop\\aicte internship folder\\afa2e701598d20110228.csv'\n",
    "try:\n",
    "    # Load with semicolon delimiter and handle decimal commas\n",
    "    df = pd.read_csv(file_path, sep=';', decimal=',')\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(\"\\nFirst few rows of the dataset:\")\n",
    "    display(df.head())\n",
    "    print(\"\\nDataset Info:\")\n",
    "    display(df.info())\n",
    "    \n",
    "    # Now apply preprocessing\n",
    "    df_clean = preprocess_data(df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"Please check the file path and format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eda(df):\n",
    "    # 1. Check target variable distribution\n",
    "    if 'Potability' in df.columns:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.countplot(x='Potability', data=df)\n",
    "        plt.title('Distribution of Water Potability')\n",
    "        plt.show()\n",
    "    \n",
    "    # 2. Correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr = df.corr()\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Distribution of numerical features\n",
    "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    for col in numerical_cols:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.histplot(data=df, x=col, kde=True)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.show()\n",
    "\n",
    "# Perform EDA if dataset is loaded and preprocessed\n",
    "if 'df_clean' in locals():\n",
    "    perform_eda(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, target_col='Potability', test_size=0.2, random_state=42):\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "\n",
    "# Prepare data if dataset is loaded and preprocessed\n",
    "if 'df_clean' in locals() and 'Potability' in df_clean.columns:\n",
    "    X_train, X_test, y_train, y_test, scaler = prepare_data(df_clean)\n",
    "    print(\"Data preparation complete!\")\n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "1. Upload your water quality dataset to the 'data' folder\n",
    "2. Update the file path in the 'Data Loading' section\n",
    "3. Run each cell sequentially to perform the analysis\n",
    "4. For Week 2, we'll focus on model building and evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
